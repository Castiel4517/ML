STACKED ML
https://blog.csdn.net/weixin_46803857/article/details/128700297

XGB
https://zhuanlan.zhihu.com/p/162001079

6. 数据集划分 -------------------------------------------------- 训练集形状: (1798, 28) 测试集形状: (450, 28) 训练集标签分布: {0: 1462, 1: 336} 测试集标签分布: {0: 366, 1: 84} 7. 机器学习模型定义 -------------------------------------------------- 定义了12种机器学习模型 8. 模型训练和评估 -------------------------------------------------- 开始训练和评估模型... 训练 Logistic Regression... ✓ 训练 SGD Classifier... ✓ 训练 Decision Tree... ✓ 训练 Random Forest... ✓ 训练 AdaBoost... ✓ 训练 Extra Trees... ✓ 训练 Support Vector Machine... ✓ 训练 Gaussian Naive Bayes... ✓ 训练 K-Nearest Neighbors... ✓ 训练 Multi-layer Perceptron... ✓ 训练 XGBoost... ✓ 训练 lightGBM... [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines [LightGBM] [Info] Number of positive: 336, number of negative: 1462 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 4304 [LightGBM] [Info] Number of data points in the train set: 1798, number of used features: 26 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.186874 -> initscore=-1.470449 [LightGBM] [Info] Start training from score -1.470449 [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines [LightGBM] [Info] Number of positive: 269, number of negative: 1169 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 4254 [LightGBM] [Info] Number of data points in the train set: 1438, number of used features: 26 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187065 -> initscore=-1.469193 [LightGBM] [Info] Start training from score -1.469193 [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines [LightGBM] [Info] Number of positive: 269, number of negative: 1169 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 4262 [LightGBM] [Info] Number of data points in the train set: 1438, number of used features: 26 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187065 -> initscore=-1.469193 [LightGBM] [Info] Start training from score -1.469193 [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines [LightGBM] [Info] Number of positive: 268, number of negative: 1170 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000184 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 4249 [LightGBM] [Info] Number of data points in the train set: 1438, number of used features: 26 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.186370 -> initscore=-1.473772 [LightGBM] [Info] Start training from score -1.473772 [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines [LightGBM] [Info] Number of positive: 269, number of negative: 1170 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 4263 [LightGBM] [Info] Number of data points in the train set: 1439, number of used features: 26 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.186935 -> initscore=-1.470048 [LightGBM] [Info] Start training from score -1.470048 [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines [LightGBM] [Info] Number of positive: 269, number of negative: 1170 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 4254 [LightGBM] [Info] Number of data points in the train set: 1439, number of used features: 26 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.186935 -> initscore=-1.470048 [LightGBM] [Info] Start training from score -1.470048 ✓ === 模型训练并缓存预测概率 === ✗ Logistic Regression 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ SGD Classifier 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ Decision Tree 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ Random Forest 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ AdaBoost 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ Extra Trees 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ Support Vector Machine 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ Gaussian Naive Bayes 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ K-Nearest Neighbors 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ Multi-layer Perceptron 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] ✗ XGBoost 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900] [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines [LightGBM] [Info] Number of positive: 336, number of negative: 1462 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 4304 [LightGBM] [Info] Number of data points in the train set: 1798, number of used features: 26 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.186874 -> initscore=-1.470449 [LightGBM] [Info] Start training from score -1.470449 ✗ lightGBM 训练或预测出错: Found input variables with inconsistent numbers of samples: [450, 900]